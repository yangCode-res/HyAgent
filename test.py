import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from typing import List

class BioBertTester:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.device = self._get_device()
        self.tokenizer = None
        self.model = None
        self._load_model()

    def _get_device(self):
        """
        è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¡ç®—è®¾å¤‡ï¼š
        1. éå†æ‰€æœ‰ NVIDIA æ˜¾å¡ã€‚
        2. è·å–æ¯å¼ å¡çš„å‰©ä½™æ˜¾å­˜ã€‚
        3. è‡ªåŠ¨é€‰æ‹©å‰©ä½™æ˜¾å­˜æœ€å¤§çš„ä¸€å¼ å¡ã€‚
        """
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            print(f"ğŸ” Found {num_gpus} GPUs available.")
            
            best_gpu_id = 0
            max_free_memory = 0
            
            # éå†æ£€æŸ¥æ¯å¼ å¡çš„æ˜¾å­˜çŠ¶æ€
            for i in range(num_gpus):
                try:
                    # mem_get_info è¿”å› (free, total) å•ä½æ˜¯å­—èŠ‚
                    free_mem, total_mem = torch.cuda.mem_get_info(i)
                    free_gb = free_mem / (1024 ** 3)
                    total_gb = total_mem / (1024 ** 3)
                    
                    print(f"   - GPU {i}: Free {free_gb:.2f} GB / Total {total_gb:.2f} GB")
                    
                    # è®°å½•å‰©ä½™æ˜¾å­˜æœ€å¤šçš„å¡
                    if free_mem > max_free_memory:
                        max_free_memory = free_mem
                        best_gpu_id = i
                except Exception as e:
                    print(f"   - GPU {i}: Check failed ({e})")

            # å¦‚æœæ‰€æœ‰å¡æ˜¾å­˜éƒ½å¾ˆå°ï¼ˆæ¯”å¦‚éƒ½å°äº1GBï¼‰ï¼Œå¯èƒ½éœ€è¦è­¦æŠ¥ï¼Œè¿™é‡Œé»˜è®¤é€‰æœ€å¤§çš„
            device_str = f"cuda:{best_gpu_id}"
            print(f"âœ… Auto-selected Device: {device_str} (Has {max_free_memory / (1024**3):.2f} GB free)")
            return torch.device(device_str)

        elif torch.backends.mps.is_available():
            print("âœ… Device: MPS (Mac M1/M2/M3)")
            return torch.device("mps")
        else:
            print("âš ï¸ Device: CPU (Slow)")
            return torch.device("cpu")

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"â³ Loading BioBERT from: {self.model_path} ...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModel.from_pretrained(self.model_path)
            
            # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¡ç®—å‡ºçš„ best device
            self.model.to(self.device)
            self.model.eval()
            print(f"âœ… Model loaded successfully on {self.device}!")
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            exit(1)

    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        if not texts:
            return np.array([])

        print(f"ğŸ”„ Encoding {len(texts)} texts...")
        
        inputs = self.tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128
        )

        # å…³é”®ï¼šç¡®ä¿æ•°æ®ä¹Ÿç§»åŠ¨åˆ°äº†é€‰å®šçš„ device (ä¾‹å¦‚ cuda:5)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            vec = outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()
        return vec

    def compute_similarity(self, vec_a, vec_b):
        vec_a = np.array(vec_a)
        vec_b = np.array(vec_b)
        norm_a = np.linalg.norm(vec_a)
        norm_b = np.linalg.norm(vec_b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        dot_product = np.dot(vec_a, vec_b)
        similarity = dot_product / (norm_a * norm_b)
        return similarity

# ==========================================
#              æµ‹è¯•å…¥å£
# ==========================================
if __name__ == "__main__":
    MODEL_PATH = "/home/nas2/path/models/biobert-base-cased-v1.1" 

    tester = BioBertTester(MODEL_PATH)

    word1 = "Aspirin"
    word2 = "Acetylsalicylic acid" 
    word3 = "Femur"
    word4 = "Fractured bone"

    all_texts = [word1, word2, word3, word4]
    
    embeddings = tester.get_embeddings(all_texts)
    
    vec1 = embeddings[0]
    vec2 = embeddings[1]
    vec3 = embeddings[2]
    vec4 = embeddings[3]

    print("\n" + "="*40)
    print("ğŸ§ª Similarity Test Results")
    print("="*40)

    sim_high = tester.compute_similarity(vec1, vec2)
    sim_low = tester.compute_similarity(vec3, vec4)
    sim_cross = tester.compute_similarity(vec1, vec3)

    print(f"1ï¸âƒ£  Pair: '{word1}' vs '{word2}'")
    print(f"   Similarity: {sim_high:.4f}")
    print("-" * 40)

    print(f"2ï¸âƒ£  Pair: '{word3}' vs '{word4}'")
    print(f"   Similarity: {sim_low:.4f}")
    print("-" * 40)
    
    print(f"3ï¸âƒ£  Pair: '{word1}' vs '{word3}'")
    print(f"   Similarity: {sim_cross:.4f}")
    print("="*40)