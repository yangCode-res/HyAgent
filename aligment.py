import json
import os
import faiss
import networkx as nx
import numpy as np
import torch
import Levenshtein  # å¿…é¡»å¼•å…¥
from dataclasses import dataclass, field, replace
from collections import defaultdict
from typing import Any, Dict, List, Optional, Tuple, Set
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer
from openai import OpenAI

# å‡è®¾è¿™æ˜¯ä½ çš„é¡¹ç›®ç»“æ„å¼•ç”¨
from Core.Agent import Agent
from Logger.index import get_global_logger
from Memory.index import Memory, load_memory_from_json
from Store.index import get_memory
from TypeDefinitions.EntityTypeDefinitions.index import KGEntity
from TypeDefinitions.TripleDefinitions.KGTriple import KGTriple
@dataclass
class TimeFormat: 
    pass



class AlignmentTripleAgent(Agent):
    def __init__(self, client: OpenAI, model_name: str, memory: Optional[Memory] = None):
        self.system_prompt = "..." 
        super().__init__(client, model_name, self.system_prompt)
        
        self.memory = memory or get_memory()
        self.logger = get_global_logger()
        self.biobert_dir = "/home/nas2/path/models/SapBERT-from-PubMedBERT-fulltext"
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModel.from_pretrained(self.biobert_dir, local_files_only=True)
        self.tokenizer = AutoTokenizer.from_pretrained(self.biobert_dir, local_files_only=True)
        self.model.eval()
        self.device = 'cpu' # å¦‚æœæœ‰ cuda æ”¹ä¸º 'cuda'
        
        self.final_triples: List[KGTriple] = []
        self.final_entities: List[KGEntity] = [] 
        self.name_mapping: Dict[str, str] = {}

    def process(self):
        # 1. æ”¶é›†åŸå§‹æ•°æ®
        raw_entities = []
        raw_triples = []
        for subgraph in self.memory.subgraphs.values():
            raw_entities.extend(subgraph.entities.all())
            raw_triples.extend(subgraph.relations.triples)

        print(f"Raw Input: {len(raw_entities)} entities, {len(raw_triples)} triples.")
        raw_entities,raw_triples=self.normalize(raw_entities, raw_triples)
        # 2. è°ƒç”¨æ•´åˆåçš„å¯¹é½å‡½æ•°
        self.final_triples, self.final_entities, _ = self.align_and_merge(
            raw_entities, 
            raw_triples, 
            threshold=0.90 # åŸºç¡€å‘é‡é˜ˆå€¼
        )
        
        # æ›´æ–° Memory
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ memory æœ‰ clear å’Œ add/upsert æ–¹æ³•
        # self.memory.entities.clear() 
        for i in self.final_entities:
            self.memory.entities.upsert(KGEntity(**i.to_dict()))
        # self.memory.entities.upsert_many(self.final_entities) 
        self.memory.relations.triples = self.final_triples
        
        return self.final_triples, self.final_entities

    def get_embeddings(self, texts, batch_size=128):
        """æ‰¹é‡è·å– SAPBERT å‘é‡"""
        if not texts: return np.array([])
        all_embs = []
        unique_texts = list(set(texts))
        text_to_idx = {t: i for i, t in enumerate(unique_texts)}
        
        for i in tqdm(range(0, len(unique_texts), batch_size), desc="Encoding entities"):
            batch = unique_texts[i : i + batch_size]
            inputs = self.tokenizer(batch, padding=True, truncation=True, 
                                  max_length=64, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
                cls_emb = outputs.last_hidden_state[:, 0, :]
                all_embs.append(cls_emb.cpu().numpy())
        
        unique_embs = np.concatenate(all_embs, axis=0)
        final_embs = np.array([unique_embs[text_to_idx[t]] for t in texts])
        return final_embs
    
    def align_and_merge(self, 
                        raw_entities: List[KGEntity], 
                        raw_triples: List[KGTriple], 
                        threshold=0.95, 
                        top_k=5) -> Tuple[List[KGTriple], List[KGEntity], Dict[str, str]]:
        
        # å®šä¹‰å—ä¿æŠ¤çš„ç±»å‹ï¼ˆå½’ä¸€åŒ–ä¸ºå°å†™ï¼‰
        # è¿™äº›ç±»å‹çš„å®ä½“å°†è·³è¿‡å‘é‡å¯¹é½ï¼Œåªå…è®¸åŸºäºæ˜¾å¼åˆ«åçš„åˆå¹¶
        PROTECTED_TYPES = {'gene', 'biomarker'}

        # --- å†…éƒ¨è¾…åŠ©å‡½æ•° ---
        def _calc_richness(ent: KGEntity) -> int:
            score = 0
            if ent.entity_type and ent.entity_type.lower() != 'unknown': score += 100
            if ent.description: score += len(ent.description)
            score += len(ent.aliases) * 10
            if ent.normalized_id and ent.normalized_id != "N/A": score += 200
            if len(ent.name) > 2: score += 10 
            return score

        def _merge_entity_list(ent_list: List[KGEntity], all_names: List[str]) -> KGEntity:
            if not ent_list: return None
            base = max(ent_list, key=_calc_richness)
            merged_aliases = set(all_names)
            merged_desc = base.description
            merged_type = base.entity_type
            merged_nid = base.normalized_id
            
            for e in ent_list:
                if e is base: continue
                if not merged_desc and e.description: merged_desc = e.description
                elif e.description and len(e.description) > len(merged_desc): merged_desc = e.description
                
                if merged_type.lower() == 'unknown' and e.entity_type.lower() != 'unknown':
                    merged_type = e.entity_type
                
                if (not merged_nid or merged_nid == "N/A") and (e.normalized_id and e.normalized_id != "N/A"):
                    merged_nid = e.normalized_id
            
            if base.name in merged_aliases: merged_aliases.remove(base.name)
            return replace(base, aliases=list(merged_aliases), description=merged_desc, entity_type=merged_type, normalized_id=merged_nid)

        # ==========================================
        # Step 1: æ„å»ºåˆ«åå›¾ & ç±»å‹ç´¢å¼•
        # ==========================================
        print("Step 1: Building Alias Graph & Type Index...")
        G = nx.Graph()
        str_to_raw_entities = defaultdict(list)
        
        # ã€æ–°å¢ã€‘: è®°å½•æ¯ä¸ªå­—ç¬¦ä¸²å…³è”çš„å®ä½“ç±»å‹é›†åˆ
        # æ ¼å¼: "BRCA1" -> {"gene"}, "IL-6" -> {"cytokine", "gene"}
        str_to_types: Dict[str, Set[str]] = defaultdict(set)

        for ent in raw_entities:
            symbols = {ent.name} | set(ent.aliases)
            symbols = {s for s in symbols if s and s.strip()}
            if not symbols: continue
            
            # è®°å½•ç±»å‹
            current_type = ent.entity_type.lower() if ent.entity_type else "unknown"
            
            for s in symbols:
                str_to_raw_entities[s].append(ent)
                G.add_node(s)
                # è®°å½•è¯¥å­—ç¬¦ä¸²å±äºä»€ä¹ˆç±»å‹
                str_to_types[s].add(current_type)
            
            # å»ºç«‹ç¡¬è¿æ¥
            symbol_list = list(symbols)
            for i in range(len(symbol_list)):
                for j in range(i + 1, len(symbol_list)):
                    G.add_edge(symbol_list[i], symbol_list[j], type='hard')

        for t in raw_triples:
            if t.head not in G: G.add_node(t.head)
            if t.tail not in G: G.add_node(t.tail)

        # ==========================================
        # Step 2: å‘é‡åŒ– (Vectorization)
        # ==========================================
        print("Step 2: Vectorizing...")
        all_node_strings = list(G.nodes())
        if not all_node_strings: return [], [], {}
        
        id2name = {i: name for i, name in enumerate(all_node_strings)}
        
        embeddings = self.get_embeddings(all_node_strings)
        embeddings = embeddings.astype(np.float32)
        embeddings = np.ascontiguousarray(embeddings)
        faiss.normalize_L2(embeddings)
        
        d = embeddings.shape[1]
        index = faiss.IndexFlatIP(d)
        index.add(embeddings)
        D, I = index.search(embeddings, int(top_k))
        
        # ==========================================
        # Step 3: æ··åˆæ ¡éªŒä¸å—ä¿æŠ¤ç±»å‹è¿‡æ»¤
        # ==========================================
        print("Step 3: Hybrid Verification with Type Guard...")
        edges_added = 0
        
        for i in range(len(all_node_strings)):
            src_name = id2name[i]
            
            # ã€æ–°å¢ã€‘æ£€æŸ¥æºå®ä½“æ˜¯å¦å—ä¿æŠ¤
            src_types = str_to_types.get(src_name, set())
            # å¦‚æœè¯¥åå­—å…³è”çš„ç±»å‹é‡ŒåŒ…å« gene æˆ– biomarkerï¼Œåˆ™ src_is_protected = True
            src_is_protected = bool(src_types & PROTECTED_TYPES)

            for j, score in zip(I[i], D[i]):
                if i == j or j == -1: continue 
                tgt_name = id2name[j]
                
                if G.has_edge(src_name, tgt_name): continue

                # ã€æ–°å¢ã€‘ç±»å‹å®ˆå«é€»è¾‘ (Type Guard)
                # æ£€æŸ¥ç›®æ ‡å®ä½“æ˜¯å¦å—ä¿æŠ¤
                tgt_types = str_to_types.get(tgt_name, set())
                tgt_is_protected = bool(tgt_types & PROTECTED_TYPES)
                
                # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœä»»ä¸€æ–¹æ˜¯ Gene/Biomarkerï¼Œç›´æ¥ç¦æ­¢å‘é‡åˆå¹¶
                # é™¤éå®ƒä»¬åŸæœ¬å°±æœ‰ç¡¬åˆ«åè¿æ¥(Step 1å·²å¤„ç†)ï¼Œå¦åˆ™ä¸è®© SapBERT æ‹‰è¿‘å®ƒä»¬
                if src_is_protected or tgt_is_protected:
                    continue

                # --- ä¸‹é¢æ˜¯å¸¸è§„æ··åˆæ ¡éªŒ ---
                should_merge = False
                
                # è§„åˆ™ A: æé«˜ç›¸ä¼¼åº¦ (éä¿æŠ¤ç±»å‹æ‰å…è®¸)
                if score >= 0.985: 
                    should_merge = True
                # è§„åˆ™ B: è¾ƒé«˜ç›¸ä¼¼åº¦ + å­—é¢ç›¸ä¼¼
                elif score >= threshold:
                    lex_sim = Levenshtein.ratio(src_name.lower(), tgt_name.lower())
                    if lex_sim > 0.5: 
                        should_merge = True
                
                if should_merge:
                    G.add_edge(src_name, tgt_name, type='soft', weight=score)
                    edges_added += 1

        print(f"Added {edges_added} semantic edges (Protected types skipped).")

        # ==========================================
        # Step 4: èšç±»ä¸åˆå¹¶ (Clustering)
        # ==========================================
        print("Step 4: Clustering & Merging...")
        clusters = list(nx.connected_components(G))
        
        final_entities = []
        name_mapping = {} 
        merged_entity_map = {}

        for cluster in clusters:
            cluster_list = list(cluster)
            
            # ç¡®å®šæ ‡å‡†å
            valid_names = [n for n in cluster_list if len(n) >= 3]
            canonical_name = sorted(valid_names, key=len)[0] if valid_names else sorted(cluster_list, key=len)[0]

            cluster_raw_entities = []
            for name_str in cluster_list:
                name_mapping[name_str] = canonical_name
                if name_str in str_to_raw_entities:
                    cluster_raw_entities.extend(str_to_raw_entities[name_str])
            
            unique_raw_entities = list({id(e): e for e in cluster_raw_entities}.values())
            
            # ================= [æ–°å¢æ‰“å°é€»è¾‘ å¼€å§‹] =================
            # åªæœ‰å½“å‘ç”Ÿäº†"åˆå¹¶"è¡Œä¸ºæ—¶æ‰æ‰“å°ï¼Œé¿å…åˆ·å±
            # æ¡ä»¶ï¼šæ¶‰åŠè¶…è¿‡1ä¸ªå®ä½“å¯¹è±¡ OR æ¶‰åŠè¶…è¿‡1ä¸ªä¸åŒçš„åå­—
            if len(unique_raw_entities) > 1 or len(cluster_list) > 1:
                print(f"\nğŸ”¹ [Merge Event] Canonical Name: '{canonical_name}'")
                
                # 1. æ‰“å°åŒä¹‰è¯ç°‡çš„æ‰€æœ‰åå­—
                print(f"   â””â”€â”€ Synonyms/Aliases ({len(cluster_list)}): {cluster_list}")
                
                # 2. æ‰“å°æ¶‰åŠçš„åŸå§‹å®ä½“å¯¹è±¡
                if len(unique_raw_entities) > 1:
                    print(f"   â””â”€â”€ âš ï¸ Merging {len(unique_raw_entities)} Distinct Entities:")
                    for idx, e in enumerate(unique_raw_entities):
                        print(f"       {idx+1}. ID: {e.entity_id:<10} | Name: {e.name:<20} | Type: {e.entity_type}")
                elif len(unique_raw_entities) == 1:
                    print(f"   â””â”€â”€ Single Entity Updated: ID {unique_raw_entities[0].entity_id} ({unique_raw_entities[0].name})")
                else:
                    print(f"   â””â”€â”€ No Entity Objects (Pure string merge from Triples)")
            # ================= [æ–°å¢æ‰“å°é€»è¾‘ ç»“æŸ] =================

            if unique_raw_entities:
                final_ent = _merge_entity_list(unique_raw_entities, cluster_list)
                if final_ent.name != canonical_name:
                    if final_ent.name not in final_ent.aliases:
                        final_ent.aliases.append(final_ent.name)
                    final_ent = replace(final_ent, name=canonical_name)
            else:
                final_ent = KGEntity(
                    entity_id=f"auto-{abs(hash(canonical_name))}",
                    name=canonical_name,
                    entity_type="Unknown",
                    aliases=[n for n in cluster_list if n != canonical_name]
                )
            
            final_entities.append(final_ent)
            merged_entity_map[canonical_name] = final_ent

        # ==========================================
        # Step 5: é‡å†™ä¸‰å…ƒç»„
        # ==========================================
        print("Step 5: Rewriting Triples...")
        final_triples = []
        seen_triples = set()
        
        for t in raw_triples:
            new_h = name_mapping.get(t.head, t.head)
            new_t = name_mapping.get(t.tail, t.tail)
            
            if new_h == new_t: continue
            
            subj_obj = merged_entity_map.get(new_h)
            obj_obj = merged_entity_map.get(new_t)
            
            triple_key = (new_h, t.relation, new_t)
            
            if triple_key not in seen_triples:
                seen_triples.add(triple_key)
                new_triple = replace(t, head=new_h, tail=new_t, subject=subj_obj, object=obj_obj)
                final_triples.append(new_triple)

        return final_triples, final_entities, name_mapping

    def normalize(self, entities: List[KGEntity], triples: List[KGTriple]):
        if not entities: # type: ignore
            return entities, triples

        # --- 1. æ„å»ºåŒä¹‰è¯è¿é€šå›¾ ---
        # èŠ‚ç‚¹ï¼šæ‰€æœ‰çš„ name å’Œ alias å­—ç¬¦ä¸²
        # è¾¹ï¼šåŒä¸€ä¸ªå®ä½“å†…çš„ name å’Œ alias ä¹‹é—´äº’è¿
        g = nx.Graph()
        
        # è®°å½•æ¯ä¸ªåå­—å¯¹åº”çš„åŸå§‹å®ä½“ï¼ˆç”¨äºåç»­æ›´æ–°å±æ€§ï¼‰
        name_to_entities = defaultdict(list)

        for ent in entities:
            # æ”¶é›†è¯¥å®ä½“æºå¸¦çš„æ‰€æœ‰åç§°ç¬¦å·
            # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
            symbols = {ent.name} | set(ent.aliases)
            symbols = {s for s in symbols if s and s.strip()}
            
            if not symbols:
                continue
                
            symbol_list = list(symbols)
            base_node = symbol_list[0]
            
            # å°†æ‰€æœ‰åå­—åŠ å…¥å›¾å¹¶è¿çº¿
            g.add_node(base_node)
            for s in symbol_list:
                name_to_entities[s].append(ent)
                if s != base_node:
                    g.add_edge(base_node, s)

        # --- 2. ç”Ÿæˆæ˜ å°„å­—å…¸ (Variant -> Canonical) ---
        # æ‰¾å‡ºè¿é€šåˆ†é‡
        clusters = list(nx.connected_components(g))
        print(f"Found {len(clusters)} explicit synonym groups.")
        
        normalization_map: Dict[str, str] = {}
        
        for cluster in clusters:
            cluster_list = list(cluster)
            
            # ç­–ç•¥ï¼šé€‰æ‹©æœ€é•¿çš„åå­—ä½œä¸ºæ ‡å‡†å
            # ä¾‹å¦‚: ["NO", "Nitric Oxide"] -> "Nitric Oxide"
            canonical_name = max(cluster_list, key=len)
            
            for name in cluster_list:
                normalization_map[name] = canonical_name

        # --- 3. é‡å†™å®ä½“ (Rewrite Entities) ---
        normalized_entities = []
        # ä½¿ç”¨ seen_ids é˜²æ­¢é‡å¤å¤„ç†åŒä¸€ä¸ªå¯¹è±¡ï¼ˆè™½ç„¶ replace ä¼šç”Ÿæˆæ–°å¯¹è±¡ï¼Œä½†è¾“å…¥åˆ—è¡¨å¯èƒ½æœ‰é‡å¤å¼•ç”¨ï¼‰
        seen_ids = set()
        
        for ent in entities:
            canon_name = normalization_map.get(ent.name, ent.name)
            
            old_names = {ent.name} | set(ent.aliases)
            new_aliases = set()
            
            for n in old_names:
                if n != canon_name:
                    new_aliases.add(n)
            
            # åˆ›å»ºæ–°å®ä½“å¯¹è±¡
            new_ent = replace(ent, 
                            name=canon_name, 
                            aliases=list(new_aliases))
            
            normalized_entities.append(new_ent)

        # --- 4. é‡å†™ä¸‰å…ƒç»„ (Rewrite Triples) ---
        normalized_triples = []
        for t in triples:
            # æŸ¥æ‰¾æ˜ å°„ï¼Œå¦‚æœæ²¡åœ¨ map é‡Œï¼ˆè¯´æ˜æ²¡åˆ«åä¿¡æ¯ï¼‰ï¼Œä¿æŒåŸæ ·
            new_head = normalization_map.get(t.head, t.head)
            new_tail = normalization_map.get(t.tail, t.tail)
            
            # å¦‚æœåå­—æ²¡å˜ï¼Œç›´æ¥å¤ç”¨ï¼›å˜äº†åˆ™ replace
            if new_head != t.head or new_tail != t.tail:
                # æ³¨æ„ï¼šè¿™é‡Œåªæ”¹äº†å­—ç¬¦ä¸²ã€‚
                # å¦‚æœ triples é‡ŒåŒ…å« subject/object å¯¹è±¡å¼•ç”¨ï¼Œæœ€å¥½ç½®ç©ºæˆ–æŒ‡å‘æ–°çš„å®ä½“ï¼Œ
                # ä½†ç”±äºå®ä½“åˆ—è¡¨ä¹Ÿé‡å»ºäº†ï¼Œè¿™é‡Œå…ˆåªå¤„ç†æ–‡æœ¬ï¼Œåç»­æµç¨‹ä¼šé‡æ–°é“¾æ¥å¯¹è±¡ã€‚
                new_t = replace(t, head=new_head, tail=new_tail)
                normalized_triples.append(new_t)
            else:
                normalized_triples.append(t)
                
        print(f"Normalized {len(normalized_entities)} entities and {len(normalized_triples)} triples.")
        return normalized_entities, normalized_triples
# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # å‡è®¾ç¯å¢ƒé…ç½®
    memory_path = '/home/nas2/path/yangmingjian/code/hygraph/snapshots/memory-20251209-231406.json'
    if os.path.exists(memory_path):
        memory = load_memory_from_json(memory_path)
        
        open_ai_api = os.environ.get("OPENAI_API_KEY")
        open_ai_url=os.environ.get("OPENAI_API_BASE_URL")
        client = OpenAI(api_key=open_ai_api, base_url=open_ai_url)
        model_name=os.environ.get("OPENAI_MODEL")

        aligner = AlignmentTripleAgent(client, model_name, memory=memory)
        
        # è¿è¡Œå¤„ç†
        triples, entities = aligner.process()

        aligner.memory.dump_json("./snapshots")
        # print("\n--- Processing Complete ---")
        # print(f"Total Merged Entities: {len(entities)}")
        # print(f"Total Merged Triples: {len(triples)}")
        
        # if len(triples) > 0:
        #     print("\nExample Triple:")
        #     t = triples[0]
        #     print(f"{t.head} --[{t.relation}]--> {t.tail}")
        #     print(f"Linked Subject Name: {t.subject.name if t.subject else 'None'}")
        #     print(f"Linked Subject Aliases: {t.subject.aliases if t.subject else 'None'}")
        