import json
import os
import faiss
import networkx as nx
import numpy as np
import torch
import Levenshtein  # å¿…é¡»å¼•å…¥
from dataclasses import dataclass, field, replace
from collections import defaultdict
from typing import Any, Dict, List, Optional, Tuple, Set
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer
from openai import OpenAI

import pandas as pd  # å¼•å…¥ pandas
# å‡è®¾è¿™æ˜¯ä½ çš„é¡¹ç›®ç»“æ„å¼•ç”¨
from Core.Agent import Agent
from Logger.index import get_global_logger
from Memory.index import Memory, load_memory_from_json
from Store.index import get_memory
from TypeDefinitions.EntityTypeDefinitions.index import KGEntity
from TypeDefinitions.TripleDefinitions.KGTriple import KGTriple

class AlignmentTripleAgent(Agent):
    def __init__(self, client: OpenAI, model_name: str, memory: Optional[Memory] = None):
        self.system_prompt = """You are an expert in biomedical knowledge graph entity alignment.

You will receive a single JSON string as user input, with fields such as:
- "candidates": a pair of objects { "src_name": ..., "tgt_name": ... }

Your task:
1. Parse the JSON input.
2. Decide whether candidates refer to the SAME real-world biomedical entity as the source entity.

Output format (VERY IMPORTANT):
- You MUST respond with STRICT JSON only.
- The JSON must have exactly one top-level key "align" which is a boolean key.
- "align" must be a list of candidate ids (strings) that should be kept.
- Example: {"align": true} means the candidate matches the source entity.

Rules:
- If no candidate should be aligned with the source entity, return {"align": false}.
- Do NOT add any other keys, text, comments, or explanations.
- Do NOT change, rename, or invent candidate ids.
- The response must be valid JSON and parseable by a standard JSON parser.""" 
        super().__init__(client, model_name, self.system_prompt)
        
        self.memory = memory or get_memory()
        self.logger = get_global_logger()
        self.biobert_dir = "/home/nas2/path/models/SapBERT-from-PubMedBERT-fulltext"
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModel.from_pretrained(self.biobert_dir, local_files_only=True)
        self.tokenizer = AutoTokenizer.from_pretrained(self.biobert_dir, local_files_only=True)
        self.model.eval()
        self.device = 'cpu' # å¦‚æœæœ‰ cuda æ”¹ä¸º 'cuda'
        
        self.final_triples: List[KGTriple] = []
        self.final_entities: List[KGEntity] = [] 
        self.name_mapping: Dict[str, str] = {}

    def process(self):
        print("--- Step 0: Data Collection ---")
        # 1. æ”¶é›†åŸå§‹æ•°æ®
        # ä½¿ç”¨å­—å…¸å»é‡ï¼Œé˜²æ­¢åŒä¸€ä¸ªå®ä½“åœ¨ä¸åŒå­å›¾ä¸­é‡å¤å‡ºç°å¯¼è‡´æ•°é‡çˆ†ç‚¸
        raw_entities_map = {}
        raw_triples = []
        
        # for subgraph in self.memory.subgraphs.values():
        #     # æ”¶é›†å®ä½“ (æŒ‰ entity_id å»é‡ï¼Œæˆ–è€…æŒ‰ name å»é‡ï¼Œè§†ä½ çš„æ•°æ®æƒ…å†µè€Œå®š)
        #     # è¿™é‡Œå‡è®¾ entity_id æ˜¯å”¯ä¸€çš„æ ‡è¯†ç¬¦
        for subgraph in self.memory.subgraphs.values():
            for ent in subgraph.entities.all():
                if ent.entity_id not in raw_entities_map:
                    raw_entities_map[ent.entity_id] = ent
        
        # æ”¶é›†ä¸‰å…ƒç»„
        raw_triples.extend(triple for subgraph in self.memory.subgraphs.values() for triple in subgraph.relations.triples)

        raw_entities = list(raw_entities_map.values())
        print(f"Raw Input: {len(raw_entities)} unique entities, {len(raw_triples)} triples.")

        # 2. é¢„å¤„ç†ï¼šåŸºäºæ˜¾å¼åˆ«åçš„å½’ä¸€åŒ– (Normalize)
        print("\n--- Step 1: Explicit Alias Normalization ---")
        normalized_entities, normalized_triples = self.normalize(raw_entities, raw_triples)
        
        # 3. æ ¸å¿ƒå¯¹é½ï¼šåŸºäº SapBERT çš„è¯­ä¹‰åˆå¹¶ (Align & Merge)
        print("\n--- Step 2: Semantic Alignment (SapBERT) ---")
        # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥çš„æ˜¯ normalize åçš„æ•°æ®
        self.final_triples, self.final_entities, _ = self.align_and_merge(
            normalized_entities, 
            normalized_triples
        )
        
        # 4. æ›´æ–° Memory
        print("\n--- Step 3: Updating Memory ---")
        # å»ºè®®å…ˆæ¸…ç©ºæ—§å®ä½“ï¼Œé¿å…æ®‹ç•™
        # self.memory.entities.clear() 
        for i in self.final_entities:
            # å‡è®¾ memory.entities.upsert æ¥å— KGEntity å¯¹è±¡
            self.memory.entities.upsert(KGEntity(**i.to_dict()))
            
        self.memory.relations.triples = self.final_triples
        
        return self.final_triples, self.final_entities

    def get_embeddings(self, texts, batch_size=128):
        """
        [ä¿®å¤ç‰ˆ] æ‰¹é‡è·å– SAPBERT å‘é‡
        ä¿®å¤äº†ç´¢å¼•é”™ä½å¯¼è‡´ä¸åŒå®ä½“è·å¾—ç›¸åŒå‘é‡(Score=1.0)çš„ä¸¥é‡Bugã€‚
        """
        if not texts: return np.array([])
        
        # 1. ç¡®å®šæ€§å»é‡ï¼šä½¿ç”¨ sorted ç¡®ä¿æ¯æ¬¡è¿è¡Œé¡ºåºä¸€è‡´ï¼Œé˜²æ­¢ set çš„éšæœºæ€§
        unique_texts = sorted(list(set(texts)))
        
        # 2. å»ºç«‹æ˜ å°„è¡¨
        text_to_idx = {t: i for i, t in enumerate(unique_texts)}
        
        all_embs = []
        
        # 3. æ‰¹é‡æ¨ç†
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for i in tqdm(range(0, len(unique_texts), batch_size), desc="Encoding unique entities"):
            batch = unique_texts[i : i + batch_size]
            
            # Tokenizer
            inputs = self.tokenizer(batch, padding=True, truncation=True, 
                                  max_length=64, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                # å– [CLS] token (batch_size, hidden_size)
                cls_emb = outputs.last_hidden_state[:, 0, :]
                all_embs.append(cls_emb.cpu().numpy())
        
        if not all_embs:
            return np.array([])
            
        unique_embs = np.concatenate(all_embs, axis=0)
        
        # 4. å®‰å…¨æ£€æŸ¥ (è‡³å…³é‡è¦)
        if len(unique_embs) != len(unique_texts):
            raise RuntimeError(f"å‘é‡ç”Ÿæˆæ•°é‡ä¸åŒ¹é…! æ–‡æœ¬æ•°: {len(unique_texts)}, å‘é‡æ•°: {len(unique_embs)}")

        # 5. æ˜ å°„å›åŸå§‹åˆ—è¡¨é¡ºåº
        try:
            final_embs = np.array([unique_embs[text_to_idx[t]] for t in texts])
        except KeyError as e:
            raise RuntimeError(f"ç´¢å¼•æ˜ å°„å¤±è´¥ï¼Œæ‰¾ä¸åˆ°é”®: {e}")
            
        return final_embs
    
    def align_and_merge(self, 
                        raw_entities: List[KGEntity], 
                        raw_triples: List[KGTriple], 
                        top_k=3) -> Tuple[List[KGTriple], List[KGEntity], Dict[str, str]]:
        
        # å®šä¹‰å—ä¿æŠ¤çš„ç±»å‹ï¼ˆå½’ä¸€åŒ–ä¸ºå°å†™ï¼‰
        # è¿™äº›ç±»å‹çš„å®ä½“å°†è·³è¿‡å‘é‡å¯¹é½ï¼Œåªå…è®¸åŸºäºæ˜¾å¼åˆ«åçš„åˆå¹¶
        PROTECTED_TYPES = {'gene', 'biomarker'}
        MIN_ENSURE_SCORE=0.99
        MIN_MIX_ENSURE_SCORE=0.96
        MIN_MIX_LEX_SCORE=0.65
        MIN_LLM_CHECKIN_SCORE=0.92
        MIN_LLM_LEX_SCORE=0.40
        # --- å†…éƒ¨è¾…åŠ©å‡½æ•° ---
        def _calc_richness(ent: KGEntity) -> int:
            score = 0
            if len(ent.name) > 2: score += 10 
            return score

        def _merge_entity_list(ent_list: List[KGEntity], all_names: List[str]) -> KGEntity:
            if not ent_list: return None
            base = max(ent_list, key=_calc_richness)
            merged_aliases = set(all_names)
            merged_desc = base.description
            merged_type = base.entity_type
            merged_nid = base.normalized_id
            
            for e in ent_list:
                if e is base: continue
                if not merged_desc and e.description: merged_desc = e.description
                elif e.description and len(e.description) > len(merged_desc): merged_desc = e.description
                
                if merged_type.lower() == 'unknown' and e.entity_type.lower() != 'unknown':
                    merged_type = e.entity_type
                
                if (not merged_nid or merged_nid == "N/A") and (e.normalized_id and e.normalized_id != "N/A"):
                    merged_nid = e.normalized_id
            
            if base.name in merged_aliases: merged_aliases.remove(base.name)
            return replace(base, aliases=list(merged_aliases), description=merged_desc, entity_type=merged_type, normalized_id=merged_nid)

        # ==========================================
        # Step 1: æ„å»ºåˆ«åå›¾ & ç±»å‹ç´¢å¼•
        # ==========================================
        print("Step 1: Building Alias Graph & Type Index...")
        G = nx.Graph()
        str_to_raw_entities = defaultdict(list)
        
        # ã€æ–°å¢ã€‘: è®°å½•æ¯ä¸ªå­—ç¬¦ä¸²å…³è”çš„å®ä½“ç±»å‹é›†åˆ
        # æ ¼å¼: "BRCA1" -> {"gene"}, "IL-6" -> {"cytokine", "gene"}
        str_to_types: Dict[str, Set[str]] = defaultdict(set)
        for ent in raw_entities:
            symbols = {ent.name} | set(ent.aliases)
            symbols = {s for s in symbols if s and s.strip()}
            if not symbols: continue
            
            # è®°å½•ç±»å‹
            current_type = ent.entity_type.lower() if ent.entity_type else "unknown"
            
            for s in symbols:
                str_to_raw_entities[s].append(ent)
                G.add_node(s)
                # è®°å½•è¯¥å­—ç¬¦ä¸²å±äºä»€ä¹ˆç±»å‹
                str_to_types[s].add(current_type)
            
            # å»ºç«‹ç¡¬è¿æ¥
            symbol_list = list(symbols)
            for i in range(len(symbol_list)):
                for j in range(i + 1, len(symbol_list)):
                    G.add_edge(symbol_list[i], symbol_list[j], type='hard')

        for t in raw_triples:
            if t.head not in G: G.add_node(t.head)
            if t.tail not in G: G.add_node(t.tail)

        # ==========================================
        # Step 2: å‘é‡åŒ– (Vectorization)
        # ==========================================
        print("Step 2: Vectorizing...")
        all_node_strings = list(G.nodes())
        if not all_node_strings: return [], [], {}
        
        id2name = {i: name for i, name in enumerate(all_node_strings)}
        
        embeddings = self.get_embeddings(all_node_strings)
        embeddings = embeddings.astype(np.float32)
        embeddings = np.ascontiguousarray(embeddings)
        faiss.normalize_L2(embeddings)
        
        d = embeddings.shape[1]
        index = faiss.IndexFlatIP(d)
        index.add(embeddings)
        D, I = index.search(embeddings, int(top_k))
        
        # ==========================================
        # Step 3: æ··åˆæ ¡éªŒä¸å—ä¿æŠ¤ç±»å‹è¿‡æ»¤
        # ==========================================
        print("Step 3: Hybrid Verification with Type Guard...")
        edges_added = 0
        
        for i in tqdm(range(len(all_node_strings))):
            src_name = id2name[i]
            
            # ã€æ–°å¢ã€‘æ£€æŸ¥æºå®ä½“æ˜¯å¦å—ä¿æŠ¤
            src_types = str_to_types.get(src_name, set())
            # å¦‚æœè¯¥åå­—å…³è”çš„ç±»å‹é‡ŒåŒ…å« gene æˆ– biomarkerï¼Œåˆ™ src_is_protected = True

            for j, score in zip(I[i], D[i]):
                if i == j or j == -1: continue 
                tgt_name = id2name[j]
                
                if G.has_edge(src_name, tgt_name): continue
                tgt_types = str_to_types.get(tgt_name, set())
                is_sensitive_pair = bool((src_types | tgt_types) & PROTECTED_TYPES)
                # ã€æ–°å¢ã€‘ç±»å‹å®ˆå«é€»è¾‘ (Type Guard)
                # æ£€æŸ¥ç›®æ ‡å®ä½“æ˜¯å¦å—ä¿æŠ¤
                
                # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœä»»ä¸€æ–¹æ˜¯ Gene/Biomarkerï¼Œç›´æ¥ç¦æ­¢å‘é‡åˆå¹¶
                # é™¤éå®ƒä»¬åŸæœ¬å°±æœ‰ç¡¬åˆ«åè¿æ¥(Step 1å·²å¤„ç†)ï¼Œå¦åˆ™ä¸è®© SapBERT æ‹‰è¿‘å®ƒä»¬

                # --- ä¸‹é¢æ˜¯å¸¸è§„æ··åˆæ ¡éªŒ ---
                should_merge = False
                
                lex_sim = Levenshtein.ratio(src_name.lower(), tgt_name.lower())
                if is_sensitive_pair:
                    if score>=0.985:
                        should_merge = True
                # è§„åˆ™ A: æé«˜ç›¸ä¼¼åº¦ (éä¿æŠ¤ç±»å‹æ‰å…è®¸)
                if score >= MIN_ENSURE_SCORE: 
                    should_merge = True
                # è§„åˆ™ B: è¾ƒé«˜ç›¸ä¼¼åº¦ + å­—é¢ç›¸ä¼¼
                elif score >= MIN_MIX_ENSURE_SCORE:
                    if lex_sim > MIN_MIX_LEX_SCORE: 
                        should_merge = True
                # è§„åˆ™ C: ä¸­é«˜åˆ† + LLM å¤æ ¸
                elif score >= MIN_LLM_CHECKIN_SCORE and lex_sim > MIN_LLM_LEX_SCORE:
                    prompt=f"""Now, please verify if the following two entity names refer to the SAME real-world biomedical entity.
                    entity 1: "{src_name}"
                    entity 2: "{tgt_name}"
                    Answer with a JSON object: {{"align": true}} if they are the same, or {{"align": false}} if they are different.
                    """
                    res=self.call_llm(prompt)
                    try:
                        data=json.loads(res)
                        if data.get("align", False):
                            should_merge = True
                    except Exception as e:
                        print(f"LLM è§£æå¤±è´¥: {e}. åŸå§‹å“åº”: {res}")

                if should_merge:
                    G.add_edge(src_name, tgt_name, type='soft', weight=score)
                    edges_added += 1

        print(f"Added {edges_added} semantic edges (Protected types skipped).")

     # ==========================================
        # Step 4: èšç±»ä¸åˆå¹¶ (Enhanced Greedy Star Strategy)
        # å¼•å…¥"ååŒ…å«"å’Œ"å…³é”®è¯äº’æ–¥"é€»è¾‘ï¼Œå½»åº•æ¶ˆé™¤å†—ä½™
        # ==========================================
        print("Step 4: Clustering (Strict Anti-Drift Strategy)...")
        
        # è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­æ˜¯å¦æ„æˆåŒ…å«å…³ç³»ï¼ˆæ³›æŒ‡ vs ç‰¹æŒ‡ï¼‰
        def _is_substring_relation(s1: str, s2: str) -> bool:
            s1_lower, s2_lower = s1.lower(), s2.lower()
            # å¦‚æœä¸€ä¸ªæ˜¯å¦ä¸€ä¸ªçš„å­ä¸²ï¼Œä¸”é•¿åº¦å·®å¼‚è¶…è¿‡ä¸€å®šæ¯”ä¾‹ï¼Œè§†ä¸ºæ³›æŒ‡å…³ç³»ï¼Œä¸å¯åˆå¹¶
            if s1_lower in s2_lower and len(s1) < len(s2) * 0.8: return True
            if s2_lower in s1_lower and len(s2) < len(s1) * 0.8: return True
            return False

        # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— Token Jaccard ç›¸ä¼¼åº¦
        def _token_jaccard(s1: str, s2: str) -> float:
            # ç§»é™¤åœç”¨è¯ï¼ˆç®€å•ç‰ˆï¼‰
            stopwords = {'of', 'and', 'in', 'the', 'with', 'to', 'for', 'a', 'an'}
            set1 = set(w for w in s1.lower().split() if w not in stopwords and len(w) > 1)
            set2 = set(w for w in s2.lower().split() if w not in stopwords and len(w) > 1)
            if not set1 or not set2: return 0.0
            return len(set1 & set2) / len(set1 | set2)


        G_work = G.copy()
        processed_nodes = set()
        
        # æ’åºç­–ç•¥ä¼˜åŒ–ï¼šä¼˜å…ˆå¤„ç†â€œé•¿â€çš„åå­—ã€‚
        # ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºé•¿åå­—é€šå¸¸æ˜¯â€œç‰¹æŒ‡â€ï¼ˆå¦‚ Acute Myocardial Infarctionï¼‰ã€‚
        # å¦‚æœå…ˆå¤„ç†çŸ­åå­—ï¼ˆå¦‚ Infarctionï¼‰ï¼Œå®ƒå®¹æ˜“æŠŠé•¿çš„å¸é™„è¿›æ¥ã€‚
        # è®©é•¿åå­—å…ˆå å±±ä¸ºç‹ï¼ŒçŸ­åå­—ï¼ˆæ³›æŒ‡ï¼‰å°±æ— æ³•åå¹¶å®ƒä»¬ã€‚
        all_nodes_sorted = sorted(G_work.nodes(), key=lambda n: len(n), reverse=True)
        
        clusters = []
        
        for center_node in all_nodes_sorted:
            if center_node in processed_nodes:
                continue
                
            current_cluster = {center_node}
            processed_nodes.add(center_node)
            
            # è·å–ç›´æ¥é‚»å±…
            neighbors = list(G_work.neighbors(center_node))
            
            for neighbor in neighbors:
                if neighbor in processed_nodes:
                    continue
                
                edge_data = G_work.get_edge_data(center_node, neighbor)
                edge_type = edge_data.get('type', 'soft')
                score = edge_data.get('weight', 0)
                
                is_safe_merge = False
                
                # --- 1. ç¡¬è¿æ¥ (Explicit Alias) ---
                if edge_type == 'hard':
                    is_safe_merge = True
                    
                # --- 2. è½¯è¿æ¥ (Vector Sim) - æä¸¥è‹›æ ¡éªŒ ---
                else:
                    # [è§„åˆ™ A]: åŒ…å«å…³ç³»é˜»æ–­ (é˜²æ³›åŒ–)
                    # ä¾‹å¦‚ "Plaque" in "Atherosclerotic Plaque" -> æ‹’ç»
                    if _is_substring_relation(center_node, neighbor):
                        is_safe_merge = False
                    
                        
                    # [è§„åˆ™ C]: æé«˜åˆ†é€šè¿‡
                    # åªæœ‰å‘é‡åˆ†æé«˜ï¼Œä¸”æ²¡æœ‰ä¸Šè¿°å†²çªæ—¶ï¼Œæ‰å…è®¸
                    elif score > 0.95: 
                        is_safe_merge = True
                        
                    # [è§„åˆ™ D]: ä¸­é«˜åˆ† + ä¸¥æ ¼ Token é‡å 
                    elif score > 0.95: # threshold å»ºè®® 0.98
                        jaccard = _token_jaccard(center_node, neighbor)
                        # è¦æ±‚ï¼šå‘é‡ç›¸ä¼¼ + å…±äº«è‡³å°‘ 60% çš„ç‰¹å¼‚æ€§ Token
                        if jaccard > 0.6: 
                            is_safe_merge = True

                if is_safe_merge:
                    current_cluster.add(neighbor)
                    processed_nodes.add(neighbor)
            
            clusters.append(list(current_cluster))

        print(f"Total clusters formed: {len(clusters)}")
        
        final_entities = []
        name_mapping = {} 
        merged_entity_map = {}

        for cluster in clusters:
            cluster_list = list(cluster)
            
            # ç¡®å®šæ ‡å‡†å
            valid_names = [n for n in cluster_list if len(n) >= 5]
            canonical_name = sorted(valid_names, key=len,reverse=True)[0] if valid_names else sorted(cluster_list, key=len)[0]

            cluster_raw_entities = []
            for name_str in cluster_list:
                name_mapping[name_str] = canonical_name
                if name_str in str_to_raw_entities:
                    cluster_raw_entities.extend(str_to_raw_entities[name_str])
            
            unique_raw_entities = list({id(e): e for e in cluster_raw_entities}.values())
            
            # ================= [æ–°å¢æ‰“å°é€»è¾‘ å¼€å§‹] =================
            # åªæœ‰å½“å‘ç”Ÿäº†"åˆå¹¶"è¡Œä¸ºæ—¶æ‰æ‰“å°ï¼Œé¿å…åˆ·å±
            # æ¡ä»¶ï¼šæ¶‰åŠè¶…è¿‡1ä¸ªå®ä½“å¯¹è±¡ OR æ¶‰åŠè¶…è¿‡1ä¸ªä¸åŒçš„åå­—
            # if len(unique_raw_entities) > 1 or len(cluster_list) > 1:
            #     print(f"\nğŸ”¹ [Merge Event] Canonical Name: '{canonical_name}'")
                
            #     # 1. æ‰“å°åŒä¹‰è¯ç°‡çš„æ‰€æœ‰åå­—
            #     print(f"   â””â”€â”€ Synonyms/Aliases ({len(cluster_list)}): {cluster_list}")
                
            #     # 2. æ‰“å°æ¶‰åŠçš„åŸå§‹å®ä½“å¯¹è±¡
            #     if len(unique_raw_entities) > 1:
            #         print(f"   â””â”€â”€ âš ï¸ Merging {len(unique_raw_entities)} Distinct Entities:")
            #         for idx, e in enumerate(unique_raw_entities):
            #             print(f"       {idx+1}. ID: {e.entity_id:<10} | Name: {e.name:<20} | Type: {e.entity_type}")
            #     elif len(unique_raw_entities) == 1:
            #         print(f"   â””â”€â”€ Single Entity Updated: ID {unique_raw_entities[0].entity_id} ({unique_raw_entities[0].name})")
            #     else:
            #         print(f"   â””â”€â”€ No Entity Objects (Pure string merge from Triples)")
            # ================= [æ–°å¢æ‰“å°é€»è¾‘ ç»“æŸ] =================

            if unique_raw_entities:
                final_ent = _merge_entity_list(unique_raw_entities, cluster_list)
                if final_ent.name != canonical_name:
                    if final_ent.name not in final_ent.aliases:
                        final_ent.aliases.append(final_ent.name)
                    final_ent = replace(final_ent, name=canonical_name)
            else:
                final_ent = KGEntity(
                    entity_id=f"auto-{abs(hash(canonical_name))}",
                    name=canonical_name,
                    entity_type="Unknown",
                    aliases=[n for n in cluster_list if n != canonical_name]
                )
            
            final_entities.append(final_ent)
            merged_entity_map[canonical_name] = final_ent

        # ==========================================
        # Step 5: é‡å†™ä¸‰å…ƒç»„
        # ==========================================
        print("Step 5: Rewriting Triples...")
        final_triples = []
        seen_triples = set()
        
        for t in raw_triples:
            new_h = name_mapping.get(t.head, t.head)
            new_t = name_mapping.get(t.tail, t.tail)
            
            if new_h == new_t: continue
            
            subj_obj = merged_entity_map.get(new_h)
            obj_obj = merged_entity_map.get(new_t)
            
            triple_key = (new_h, t.relation, new_t)
            
            if triple_key not in seen_triples:
                seen_triples.add(triple_key)
                new_triple = replace(t, head=new_h, tail=new_t, subject=subj_obj, object=obj_obj)
                final_triples.append(new_triple)

        return final_triples, final_entities, name_mapping

    def normalize(self, entities: List[KGEntity], triples: List[KGTriple]):
        if not entities: # type: ignore
            return entities, triples

        # --- 1. æ„å»ºåŒä¹‰è¯è¿é€šå›¾ ---
        # èŠ‚ç‚¹ï¼šæ‰€æœ‰çš„ name å’Œ alias å­—ç¬¦ä¸²
        # è¾¹ï¼šåŒä¸€ä¸ªå®ä½“å†…çš„ name å’Œ alias ä¹‹é—´äº’è¿
        g = nx.Graph()
        
        # è®°å½•æ¯ä¸ªåå­—å¯¹åº”çš„åŸå§‹å®ä½“ï¼ˆç”¨äºåç»­æ›´æ–°å±æ€§ï¼‰
        name_to_entities = defaultdict(list)

        for ent in entities:
            # æ”¶é›†è¯¥å®ä½“æºå¸¦çš„æ‰€æœ‰åç§°ç¬¦å·
            # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
            symbols = {ent.name} | set(ent.aliases)
            symbols = {s for s in symbols if s and s.strip()}
            
            if not symbols:
                continue
                
            symbol_list = list(symbols)
            base_node = symbol_list[0]
            
            # å°†æ‰€æœ‰åå­—åŠ å…¥å›¾å¹¶è¿çº¿
            g.add_node(base_node)
            for s in symbol_list:
                name_to_entities[s].append(ent)
                if s != base_node:
                    g.add_edge(base_node, s)

        # --- 2. ç”Ÿæˆæ˜ å°„å­—å…¸ (Variant -> Canonical) ---
        # æ‰¾å‡ºè¿é€šåˆ†é‡
        clusters = list(nx.connected_components(g))
        print(f"Found {len(clusters)} explicit synonym groups.")
        
        normalization_map: Dict[str, str] = {}
        
        for cluster in clusters:
            cluster_list = list(cluster)
            
            # ç­–ç•¥ï¼šé€‰æ‹©æœ€é•¿çš„åå­—ä½œä¸ºæ ‡å‡†å
            # ä¾‹å¦‚: ["NO", "Nitric Oxide"] -> "Nitric Oxide"
            canonical_name = max(cluster_list, key=len)
            
            for name in cluster_list:
                normalization_map[name] = canonical_name

        # --- 3. é‡å†™å®ä½“ (Rewrite Entities) ---
        normalized_entities = []
        # ä½¿ç”¨ seen_ids é˜²æ­¢é‡å¤å¤„ç†åŒä¸€ä¸ªå¯¹è±¡ï¼ˆè™½ç„¶ replace ä¼šç”Ÿæˆæ–°å¯¹è±¡ï¼Œä½†è¾“å…¥åˆ—è¡¨å¯èƒ½æœ‰é‡å¤å¼•ç”¨ï¼‰
        seen_ids = set()
        
        for ent in entities:
            canon_name = normalization_map.get(ent.name, ent.name)
            
            old_names = {ent.name} | set(ent.aliases)
            new_aliases = set()
            
            for n in old_names:
                if n != canon_name:
                    new_aliases.add(n)
            
            # åˆ›å»ºæ–°å®ä½“å¯¹è±¡
            new_ent = replace(ent, 
                            name=canon_name, 
                            aliases=list(new_aliases))
            
            normalized_entities.append(new_ent)

        # --- 4. é‡å†™ä¸‰å…ƒç»„ (Rewrite Triples) ---
        normalized_triples = []
        if not triples:
            return normalized_entities, normalized_triples
        for t in triples:
            # æŸ¥æ‰¾æ˜ å°„ï¼Œå¦‚æœæ²¡åœ¨ map é‡Œï¼ˆè¯´æ˜æ²¡åˆ«åä¿¡æ¯ï¼‰ï¼Œä¿æŒåŸæ ·
            new_head = normalization_map.get(t.head, t.head)
            new_tail = normalization_map.get(t.tail, t.tail)
            
            # å¦‚æœåå­—æ²¡å˜ï¼Œç›´æ¥å¤ç”¨ï¼›å˜äº†åˆ™ replace
            if new_head != t.head or new_tail != t.tail:
                # æ³¨æ„ï¼šè¿™é‡Œåªæ”¹äº†å­—ç¬¦ä¸²ã€‚
                # å¦‚æœ triples é‡ŒåŒ…å« subject/object å¯¹è±¡å¼•ç”¨ï¼Œæœ€å¥½ç½®ç©ºæˆ–æŒ‡å‘æ–°çš„å®ä½“ï¼Œ
                # ä½†ç”±äºå®ä½“åˆ—è¡¨ä¹Ÿé‡å»ºäº†ï¼Œè¿™é‡Œå…ˆåªå¤„ç†æ–‡æœ¬ï¼Œåç»­æµç¨‹ä¼šé‡æ–°é“¾æ¥å¯¹è±¡ã€‚
                new_t = replace(t, head=new_head, tail=new_tail)
                normalized_triples.append(new_t)
            else:
                normalized_triples.append(t)
                
        print(f"Normalized {len(normalized_entities)} entities and {len(normalized_triples)} triples.")
        return normalized_entities, normalized_triples
# --- ä½¿ç”¨ç¤ºä¾‹ ---

if __name__ == "__main__":
    # å‡è®¾ç¯å¢ƒé…ç½®
    memory_path = '/home/nas3/biod/dongkun/snapshots/memory-20251210-171318.json'
    if os.path.exists(memory_path):
        memory = load_memory_from_json(memory_path)
        
        open_ai_api = os.environ.get("OPENAI_API_KEY")
        open_ai_url=os.environ.get("OPENAI_API_BASE_URL")
        client = OpenAI(api_key=open_ai_api, base_url=open_ai_url)
        model_name=os.environ.get("OPENAI_MODEL")

        aligner = AlignmentTripleAgent(client, model_name, memory=memory)

        # è¿è¡Œå¤„ç†
        triples, entities = aligner.process()

        aligner.memory.dump_json("./snapshots")
        print("\n--- Processing Complete ---")
        print(f"Total Merged Entities: {len(memory.entities.all())}")
        print(f"Total Merged Triples: {len(memory.relations.triples)}")
        
        # if len(triples) > 0:
        #     print("\nExample Triple:")
        #     t = triples[0]
        #     print(f"{t.head} --[{t.relation}]--> {t.tail}")
        #     print(f"Linked Subject Name: {t.subject.name if t.subject else 'None'}")
        #     print(f"Linked Subject Aliases: {t.subject.aliases if t.subject else 'None'}")
        