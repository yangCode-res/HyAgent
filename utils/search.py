# -*- coding: utf-8 -*-  # æŒ‡å®šæ–‡ä»¶ç¼–ç ä¸ºUTF-8ï¼Œæ”¯æŒä¸­æ–‡
"""
Time-sliced PubMed Review Search with LLM query generation.  # æ–‡ä»¶è¯´æ˜ï¼šæŒ‰æ—¶é—´åˆ‡ç‰‡çš„ç»¼è¿°æ£€ç´¢ï¼Œå«LLMç”Ÿæˆæ£€ç´¢å¼
Only searches and ranks review articles; does NOT fetch full text.  # ä»…æ£€ç´¢å’Œæ’åºç»¼è¿°ï¼Œä¸è·å–å…¨æ–‡
Requires: metapub, your api.generate_text (LLM).  # ä¾èµ–metapubä¸ä½ çš„LLMæ¥å£api.generate_text
Optional: set env NCBI_API_KEY to improve E-utilities rate limits.  # å¯è®¾ç½®NCBI_API_KEYæå‡é€Ÿç‡é™åˆ¶
"""

import logging  # æ—¥å¿—è®°å½•
import math  # æ•°å­¦å‡½æ•°åº“ï¼Œç”¨äºsqrtã€tanhç­‰
import time  # æ—¶é—´ä¸sleep
from collections import defaultdict  # æä¾›é»˜è®¤å­—å…¸ç»“æ„
from datetime import datetime  # è·å–å½“å‰å¹´ä»½ç­‰
from typing import Callable, Dict, List, Optional, Tuple  # ç±»å‹æ³¨è§£

from tqdm import tqdm  # è¿›åº¦æ¡æ˜¾ç¤º

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

from metapub import PubMedFetcher  # å¼•å…¥PubMedæ£€ç´¢å™¨

fetch = PubMedFetcher()  # åˆ›å»ºå…¨å±€fetchå®ä¾‹ï¼ˆå†…éƒ¨æœ‰ç¼“å­˜ï¼‰
# from api import generate_text  # åŠ¨æ€å¯¼å…¥ä½ çš„LLMæ¥å£
# -------------------- 1) LLM ç”Ÿæˆ PubMed æ£€ç´¢å¼ --------------------
def llm_query_from_user_question(user_query: str) -> str:  # å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬ä¸ºPubMedæ£€ç´¢å¼
    """
    ç”¨ generate_text æŠŠè‡ªç„¶è¯­è¨€é—®é¢˜è½¬æˆ PubMed æ£€ç´¢å¼ï¼ˆä»…è¿”å›æ£€ç´¢å¼ï¼‰ã€‚  #
    ä¸è¦åœ¨è¿™é‡ŒåŠ æ—¥æœŸï¼Œæ—¥æœŸäº¤ç»™æ„é€ å‡½æ•°ç”¨ [dp] åšåˆ‡ç‰‡ã€‚  # æ—¥æœŸèŒƒå›´ç”±åç»­build_queryæ·»åŠ 
    """
    
    prompt = f"""  
            ä½ æ˜¯èµ„æ·±ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯æ£€ç´¢ä¸“å®¶ã€‚è¯·ä¸ºä¸‹è¿°ç ”ç©¶é—®é¢˜ç”Ÿæˆ PubMed æ£€ç´¢å¼ï¼ˆBoolean + MeSH + è‡ªç”±è¯ï¼‰ï¼š
            é—®é¢˜ï¼š{user_query}
            è¦æ±‚ï¼š
            1) åŒæ—¶ä½¿ç”¨ MeSH ä¸è‡ªç”±è¯ï¼ˆTitle/Abstract ç”¨ [tiab]ï¼‰
            2) ä½¿ç”¨å¸ƒå°”è¿ç®—ç¬¦ AND/OR/NOT
            3) é™å®šæ–‡çŒ®ç±»å‹ä¸ºç»¼è¿°ï¼ˆReview[Publication Type]ï¼‰ï¼Œä½†ä¸è¦å†™æ—¥æœŸèŒƒå›´
            4) åªè¾“å‡ºæ£€ç´¢å¼æœ¬èº«ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€æ ‡ç‚¹ä¿®é¥°æˆ–ä»£ç å—
                """.strip()  # å»æ‰å‰åç©ºç™½
    q = str(generate_text(prompt)).strip()  # è°ƒLLMç”Ÿæˆæ£€ç´¢å¼å¹¶å»ç©ºç™½
    for bad in ("```", "â€œ", "â€"):  # æ¸…ç†å¸¸è§åŒ…è£¹ç¬¦å·ï¼ˆä»£ç å—/ä¸­æ–‡å¼•å·ï¼‰
        q = q.replace(bad, "")  # æ›¿æ¢ä¸ºæ— 
    return q  # è¿”å›æ£€ç´¢å¼å­—ç¬¦ä¸²

# -------------------- 2) æ„é€ æ£€ç´¢å¼ï¼ˆå åŠ ç±»å‹+æ—¥æœŸ+è¯­è¨€ï¼‰ --------------------
def build_query(base_query: str, y_from: int, y_to: int,  # æ„é€ æœ€ç»ˆæ£€ç´¢å¼
                strong_review: bool = False,  # æ˜¯å¦ä½¿ç”¨æ›´å¼ºçš„ç»¼è¿°ç±»å‹é›†åˆ
                lang_filter: Optional[List[str]] = None) -> str:  # è¯­è¨€è¿‡æ»¤åˆ—è¡¨
    review_clause = '(Review[Publication Type])'  # åŸºæœ¬ç»¼è¿°é™å®š
    if strong_review:  # è‹¥è¦æ±‚æ›´å¼ºç»¼è¿°
        review_clause = '(' + ' OR '.join([  # åˆå¹¶Review/Systematic/Meta-Analysisä¸‰ç±»
            'Review[Publication Type]',
            'Systematic Review[Publication Type]',
            'Meta-Analysis[Publication Type]'
        ]) + ')'  # ç»“æŸæ‹¬å·
    date_clause = f'("{y_from}/01/01"[dp] : "{y_to}/12/31"[dp])'  # å‡ºç‰ˆæ—¥æœŸåŒºé—´[dp]ï¼ŒæŒ‰å¹´åˆ‡ç‰‡
    lang_clause = ''  # é»˜è®¤æ— è¯­è¨€çº¦æŸ
    if lang_filter:  # å¦‚æœä¼ å…¥è¯­è¨€è¿‡æ»¤
        langs = ' OR '.join([f'{l}[lang]' for l in lang_filter])  # ç»„åˆæˆ OR è¡¨è¾¾å¼
        lang_clause = f' AND ({langs})'  # æ‹¼æ¥åˆ°æ£€ç´¢å¼
    return f'({base_query}) AND {review_clause} AND {date_clause}{lang_clause}'  # è¿”å›å®Œæ•´æ£€ç´¢å¼

# -------------------- 3) åˆ†é¡µæ‹‰ PMID --------------------
def paginate_pmids(query: str, quota: int, page: int = 250) -> List[str]:  # åˆ†é¡µè·å–PMID
    """
    ç”¨ retstart åˆ†é¡µæŠ“ PMIDï¼Œç›´åˆ°è¾¾åˆ° quota æˆ–æ— æ›´å¤šç»“æœã€‚  # å‡½æ•°è¯´æ˜
    """
    pmids: List[str] = []  # å­˜æ”¾ç»“æœPMIDåˆ—è¡¨
    retstart = 0  # åˆå§‹åŒ–åˆ†é¡µèµ·ç‚¹
    while len(pmids) < quota:  # æœªè¾¾åˆ°é…é¢åˆ™ç»§ç»­
        chunk = fetch.pmids_for_query(query, retmax=min(page, quota - len(pmids)), retstart=retstart)  # æ‹‰ä¸€é¡µPMID
        if not chunk:  # è‹¥æ— ç»“æœ
            break  # ç»ˆæ­¢å¾ªç¯
        pmids.extend(chunk)  # è¿½åŠ åˆ°ç»“æœåˆ—è¡¨
        if len(chunk) < min(page, quota - len(pmids)):  # è‹¥è¿”å›ä¸è¶³ä¸€é¡µï¼Œè¯´æ˜æ²¡æ›´å¤šäº†
            break  # è·³å‡º
        retstart += len(chunk)  # ç§»åŠ¨åˆ†é¡µèµ·ç‚¹
    return pmids  # è¿”å›PMIDåˆ—è¡¨

# -------------------- 4) æ‰“åˆ†ç»„ä»¶ï¼ˆæ–°è¿‘æ€§ + å½±å“åŠ› + å¯é€‰ç›¸å…³æ€§ï¼‰ --------------------
def recency_norm(pubdate: str, year_min: int, year_max: int) -> float:  # è®¡ç®—æ–°è¿‘æ€§å½’ä¸€åŒ–åˆ†
    try:
        y = int(str(pubdate)[:4])  # ä»pubdateæˆªå–å¹´ä»½
    except Exception:
        return 0.5  # ç¼ºå¤±å¹´ä»½ç»™ä¸­æ€§å€¼
    y = max(min(y, year_max), year_min)  # é™åˆ¶åœ¨èŒƒå›´å†…
    return (y - year_min) / max(1, (year_max - year_min))  # çº¿æ€§æ˜ å°„åˆ°[0,1]

def impact_norm_by_year(pmid: str, year: int,  # ä¼°ç®—å½±å“åŠ›ï¼šåŒå¹´å†…åšzscore
                        cache: Dict[str, int],  # ç¼“å­˜æ¯ç¯‡è¢«å¼•è¿‘ä¼¼å€¼
                        year_stats: Dict[int, List[int]]) -> float:  # è®°å½•æ¯å¹´è¢«å¼•è®¡æ•°åˆ†å¸ƒ
    """
    ç”¨ related_pmids(pmid).get('citedin', []) çš„æ•°é‡è¿‘ä¼¼â€œå½±å“åŠ›â€ï¼Œ  # æ–¹æ³•è¯´æ˜
    ç»Ÿè®¡å€¼æ”¾åˆ°å¯¹åº”å¹´ä»½çš„åˆ†å¸ƒï¼Œç”¨äºåç»­ zscoreã€‚  # ä¾›zscoreæ ‡å‡†åŒ–
    """
    if pmid not in cache:  # è‹¥ç¼“å­˜æ²¡æœ‰
        try:
            rel = fetch.related_pmids(pmid) or {}  # è·å–ç›¸å…³æ–‡çŒ®å­—å…¸
            cited = rel.get('citedin', []) or []  # å–citedinåˆ—è¡¨ï¼Œè¿‘ä¼¼è¢«å¼•/ç›¸ä¼¼å¼ºåº¦
            cache[pmid] = len(cited)  # è®°å½•è¢«å¼•æ•°é‡
        except Exception:
            cache[pmid] = 0  # å¼‚å¸¸åˆ™è®°0
    year_stats.setdefault(year, []).append(cache[pmid])  # å°†è¯¥æ•°å€¼åŠ å…¥å¯¹åº”å¹´ä»½åˆ†å¸ƒ
    return float(cache[pmid])  # è¿”å›åŸå§‹å½±å“åŠ›å€¼

def zscore(x: float, arr: List[float]) -> float:  # è®¡ç®—zåˆ†æ•°
    if not arr:  # è‹¥åˆ†å¸ƒä¸ºç©º
        return 0.0  # è¿”å›0
    mu = sum(arr) / len(arr)  # å‡å€¼
    var = sum((a - mu) ** 2 for a in arr) / max(1, len(arr) - 1)  # æ–¹å·®ï¼ˆæ— åä¼°è®¡ï¼‰
    std = math.sqrt(var) if var > 0 else 1.0  # æ ‡å‡†å·®ï¼Œé¿å…é™¤0
    return (x - mu) / std  # è¿”å›zåˆ†æ•°

def score_article(article,  # è®¡ç®—æ–‡ç« ç»¼åˆåˆ†
                  cited_cache: Dict[str, int],  # è¢«å¼•ç¼“å­˜
                  year_stats: Dict[int, List[int]],  # æ¯å¹´è¢«å¼•åˆ†å¸ƒ
                  y_min: int, y_max: int,  # å¹´ä»½èŒƒå›´
                  alpha: float = 0.6,  # æ–°è¿‘æ€§æƒé‡
                  beta: float = 0.4) -> float:  # å½±å“åŠ›æƒé‡
    """
    ç»¼åˆåˆ† = æ–°è¿‘æ€§(Î±) + å½±å“åŠ›(Î²)  # è¯„åˆ†å…¬å¼è¯´æ˜
    """
    try:
        y = int(str(article.pubdate)[:4])  # æå–å¹´ä»½
    except Exception:
        y = y_min  # ç¼ºå¤±å¹´ä»½æŒ‰æœ€å°å¹´å¤„ç†
    r = recency_norm(str(article.pubdate), y_min, y_max)  # è®¡ç®—æ–°è¿‘æ€§åˆ†
    imp_raw = impact_norm_by_year(article.pmid, y, cited_cache, year_stats)  # è·å–åŸå§‹å½±å“åŠ›å€¼
    imp_z = zscore(imp_raw, year_stats.get(y, []))  # è½¬ä¸ºæŒ‰å¹´zåˆ†
    return alpha * r + beta * (0.5 + 0.5 * math.tanh(imp_z))  # è¿”å›ç»¼åˆåˆ†ï¼ˆå¯¹zåˆ†ç”¨tanhå¹³æ»‘ï¼‰

# -------------------- 5) å¹´ä»½é…é¢ä¸å¤šæ ·åŒ– --------------------
def allocate_quota(years: List[int], batch_size: int,  # ä¸ºæ¯å¹´åˆ†é…å€™é€‰é…é¢
                   lambda_decay: float = 0.35, min_floor: int = 6) -> Dict[int, int]:
    """
    æŒ‡æ•°è¡°å‡é…é¢ + åœ°æ¿é…é¢ï¼ˆè¿‘æœŸå¹´æƒé‡å¤§ï¼‰ã€‚  # åˆ†é…ç­–ç•¥è¯´æ˜
    """
    weights = {y: math.exp(-lambda_decay * (max(years) - y)) for y in years}  # è®¡ç®—æ¯å¹´çš„æƒé‡ï¼ˆè¶Šè¿‘è¶Šå¤§ï¼‰
    s = sum(weights.values()) or 1.0  # æƒé‡å’Œï¼Œé¿å…0
    alloc = {y: max(min_floor, int(batch_size * (weights[y] / s))) for y in years}  # åˆå§‹æŒ‰æƒé‡åˆ†é…å¹¶åŠ åœ°æ¿
    total = sum(alloc.values())  # è®¡ç®—æ€»é…é¢
    if total > batch_size:  # è‹¥è¶…å‡ºbatch_size
        ratio = batch_size / total  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        for y in sorted(years, reverse=True):  # ä»è¿‘åˆ°è¿œè°ƒæ•´
            if sum(alloc.values()) <= batch_size:  # è¾¾æ ‡åˆ™åœæ­¢
                break  # ç»“æŸå¾ªç¯
            if alloc[y] > min_floor:  # ä»…ç¼©å‡é«˜äºåœ°æ¿çš„å¹´ä»½
                delta = max(1, int((alloc[y] - min_floor) * (1 - ratio)))  # è®¡ç®—ç¼©å‡é‡
                alloc[y] = max(min_floor, alloc[y] - delta)  # åº”ç”¨ç¼©å‡ä¸”ä¸ä½äºåœ°æ¿
    return alloc  # è¿”å›æ¯å¹´çš„é…é¢å­—å…¸

def diversified_topk(candidates: List[Tuple[float, int, object]],  # å€™é€‰ä¸º(åˆ†æ•°, å¹´ä»½, æ–‡ç« å¯¹è±¡)
                     K: int, year_max_ratio: float = 0.4):  # é€‰TopKå¹¶é™åˆ¶å•å¹´å æ¯”
    """
    é˜²æ­¢åŒä¸€å¹´â€œåˆ·å±â€ã€‚cap=âŒŠK*year_max_ratioâŒ‹ï¼Œä¸è¶³å†æ— æ¡ä»¶è¡¥é½ã€‚  # å‡½æ•°è¯´æ˜
    candidates: [(score, year, article)] å·²æŒ‰åˆ†æ•°é™åº  # å‚æ•°è¯´æ˜
    """
    by_year = defaultdict(int)  # è®°å½•æ¯å¹´å·²é€‰æ•°é‡
    out = []  # æœ€ç»ˆè¾“å‡ºåˆ—è¡¨
    cap = max(1, int(K * year_max_ratio))  # å•å¹´æœ€å¤§å æ¯”ä¸Šé™ï¼ˆè‡³å°‘1ï¼‰
    for s, y, a in candidates:  # éå†é™åºå€™é€‰
        if len(out) >= K:  # å¦‚æœå·²æ»¡K
            break  # ç»“æŸ
        if by_year[y] >= cap:  # è‹¥è¯¥å¹´å·²è¾¾ä¸Šé™
            continue  # è·³è¿‡è¯¥é¡¹
        out.append((s, y, a))  # æ¥å—è¯¥å€™é€‰
        by_year[y] += 1  # è®¡æ•°+1
    if len(out) < K:  # è‹¥ä¸è¶³K
        seen = {id(a) for _, _, a in out}  # å·²é€‰æ–‡ç« çš„idé›†åˆ
        for s, y, a in candidates:  # å†æ¬¡éå†
            if len(out) >= K:  # å¡«æ»¡åˆ™åœ
                break  # è·³å‡º
            if id(a) in seen:  # è·³è¿‡å·²é€‰
                continue  # ç»§ç»­
            out.append((s, y, a))  # æ— æ¡ä»¶è¡¥é½
    return out[:K]  # è¿”å›TopKå€™é€‰

# -------------------- 6) æ ¸å¿ƒï¼šä»…æ£€ç´¢ç»¼è¿°çš„æ‰¹æ¬¡æœç´¢ --------------------
def batch_search_reviews(base_query: str,  # åŸºç¡€ä¸»é¢˜æ£€ç´¢å¼ï¼ˆä¸å«æ—¥æœŸï¼‰
                         years_back: int = 10,  # å›æº¯å¹´æ•°ï¼ˆè¿‘Nå¹´ï¼‰
                         batch_size: int = 60,  # æ¯æ‰¹å€™é€‰æ€»é…é¢
                         topk_batch: int = 10,  # æ¯æ‰¹è¾“å‡ºTopK
                         K_total: int = 30,  # æ€»å…±å¸Œæœ›å¾—åˆ°çš„æ¡æ•°
                         strong_review: bool = False,  # æ˜¯å¦ä½¿ç”¨å¼ºç»¼è¿°ç±»å‹
                         lang_filter: Optional[List[str]] = None,  # è¯­è¨€è¿‡æ»¤
                         lambda_decay: float = 0.35,  # å¹´ä»½æƒé‡æŒ‡æ•°è¡°å‡å‚æ•°
                         year_max_ratio: float = 0.4):  # å•å¹´å æ¯”ä¸Šé™
    
    start_time = time.time()
    logger.info("ğŸš€ Starting batch search for reviews...")
    logger.info(f"ğŸ“Š Parameters: years_back={years_back}, batch_size={batch_size}, K_total={K_total}")
    """
    è¿”å›ç»“æ„åŒ–åˆ—è¡¨ï¼ˆä¸å–å…¨æ–‡ï¼‰ï¼š  # è¿”å›ç»“æœè¯´æ˜
      [
        {'pmid': str, 'title': str, 'pubdate': str, 'journal': str,
         'score': float, 'year': int, 'mesh': List[str] | None},
        ...
      ]
    """
    this_year = datetime.now().year  # å½“å‰å¹´ä»½
    years = list(range(this_year - years_back + 1, this_year + 1))  # æ„é€ å¹´ä»½åˆ—è¡¨ï¼ˆå«ä»Šå¹´ï¼‰

    selected: List[Dict] = []  # å·²é€‰ç»“æœåˆ—è¡¨ï¼ˆç»“æ„åŒ–å­—å…¸ï¼‰
    selected_pmids: set = set()  # å·²é€‰PMIDé›†åˆï¼ˆå»é‡ç”¨ï¼‰
    seen_pmids: set = set()  # å·²è§è¿‡çš„PMIDé›†åˆï¼ˆè·¨æ‰¹å»é‡ï¼‰
    cited_cache: Dict[str, int] = {}  # å½±å“åŠ›ç¼“å­˜ï¼špmid->è¢«å¼•è¿‘ä¼¼æ•°
    year_stats: Dict[int, List[int]] = {}  # æ¯å¹´è¢«å¼•åˆ†å¸ƒç”¨äºzscore

    batch_count = 0
    while len(selected) < K_total:  # è‹¥æœªè¾¾åˆ°æ€»ç›®æ ‡
        batch_count += 1
        batch_start_time = time.time()
        logger.info(f"\nğŸ”„ Starting batch {batch_count} (current progress: {len(selected)}/{K_total})")

        alloc = allocate_quota(years, batch_size=batch_size,  # åˆ†é…æœ¬æ‰¹å„å¹´ä»½é…é¢
                               lambda_decay=lambda_decay, min_floor=6)  # ä½¿ç”¨æŒ‡æ•°è¡°å‡+åœ°æ¿
        logger.info(f"ğŸ“Š Year quota allocation: {dict(alloc)}")

        pool: List[Tuple[float, int, object]] = []  # æœ¬æ‰¹å€™é€‰æ± ï¼ˆåˆ†æ•°, å¹´ä»½, æ–‡ç« ï¼‰
        with tqdm(total=len(years), desc=f"Processing years", unit="year") as pbar:
            for y in years:  # éå†æ¯ä¸ªå¹´ä»½
                query = build_query(base_query, y, y,  # æ„é€ è¯¥å¹´çš„æœ€ç»ˆæ£€ç´¢å¼ï¼ˆå¸¦Review/æ—¥æœŸ/è¯­è¨€ï¼‰
                                    strong_review=strong_review, lang_filter=lang_filter)  # å‚æ•°ä¼ é€’
                quota = alloc[y]  # ä½¿ç”¨åŸå§‹é…é¢
                logger.info(f"\nğŸ“… Processing year {y} (quota: {quota})")

                # å¦‚æœå·²ç»å¤„ç†è¿‡è¿™ä¸€å¹´ä½†è¿˜éœ€è¦æ›´å¤šæ–‡ç« ï¼Œå¢åŠ é…é¢
                if len(selected) < K_total and any(int(str(r.pubdate)[:4]) == y for r in selected):
                    extra_quota = min(quota * 2, 50)  # æœ€å¤šé¢å¤–å¢åŠ 50ç¯‡
                    logger.info(f"  â„¹ï¸ Adding extra quota (+{extra_quota}) to find more articles")
                    quota += extra_quota

                pmids = paginate_pmids(query, quota=quota, page=250)  # åˆ†é¡µæŠ“PMID
                if pmids:
                    logger.info(f"  ğŸ“‘ Found {len(pmids)} articles for year {y}")
                    with tqdm(total=len(pmids), desc=f"Processing articles", unit="article") as article_pbar:
                        for pmid in pmids:  # éå†è¯¥å¹´PMID
                            if pmid in seen_pmids:  # è‹¥å·²å¤„ç†
                                continue  # è·³è¿‡
                            try:
                                a = fetch.article_by_pmid(pmid)  # è·å–æ–‡ç« å…ƒæ•°æ®
                                if not a:  # è‹¥ä¸ºç©º
                                    continue  # è·³è¿‡
                                
                                yy = int(str(a.pubdate)[:4])  # æå–å¹´ä»½
                                s = score_article(a, cited_cache, year_stats,  # è®¡ç®—ç»¼åˆåˆ†
                                              min(years), max(years))
                                pool.append((s, yy, a))  # åŠ å…¥å€™é€‰æ± 
                                seen_pmids.add(pmid)  # æ ‡è®°å·²è§
                                
                                    
                                article_pbar.update(1)  # æ›´æ–°æ–‡ç« è¿›åº¦æ¡
                                
                            except Exception as e:
                                logger.debug(f"  âš ï¸ Failed to process article {pmid}: {str(e)}")
                                continue  # å¼‚å¸¸å¿½ç•¥
                                
                pbar.update(1)  # æ›´æ–°å¹´ä»½è¿›åº¦æ¡

        if not pool:  # è‹¥æœ¬æ‰¹æ— å€™é€‰
            logger.info("âš ï¸ No candidates found in this batch")
            if len(selected) < K_total:
                logger.info(f"ğŸ”„ Resetting seen_pmids to try finding more articles (have {len(selected)}, need {K_total})")
                seen_pmids.clear()  # æ¸…é™¤å·²è§æ ‡è®°ï¼Œå…è®¸é‡æ–°å¤„ç†ä¹‹å‰çš„æ–‡ç« 
                continue  # ç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡
            break  # ç»“æŸå¾ªç¯

        logger.info(f"\nğŸ”„ Processing batch results (found {len(pool)} candidates)")
        pool.sort(key=lambda x: x[0], reverse=True)  # æŒ‰åˆ†æ•°é™åºæ’åºå€™é€‰æ± 
        # è®¡ç®—è¿™ä¸€æ‰¹éœ€è¦é€‰æ‹©å¤šå°‘æ–‡ç« 
        remaining = K_total - len(selected)
        current_batch_size = min(remaining * 2, topk_batch * 2)  # é€‰æ‹©æ›´å¤šå€™é€‰ï¼Œä½†ä¸è¶…è¿‡ä¸¤å€çš„topk_batch
        batch_pick = diversified_topk(pool, current_batch_size, year_max_ratio=year_max_ratio)  # åº”ç”¨å¤šæ ·åŒ–å–TopK
        logger.info(f"âœ… Selected {len(batch_pick)} articles after diversity filtering (aiming for {remaining} more)")

        added_count = 0
        for score, yy, art in batch_pick:  # éå†æœ¬æ‰¹é€‰å‡ºçš„æ–‡ç« 
            if len(selected) >= K_total:  # å¦‚æœå·²æ»¡è¶³æ€»é‡
                break  # åœæ­¢æ·»åŠ 
            if art.pmid in selected_pmids:  # è‹¥è¯¥ç¯‡å·²åœ¨æœ€ç»ˆç»“æœ
                continue  # è·³è¿‡
            # å°†åˆ†æ•°æ·»åŠ åˆ°articleå¯¹è±¡
            setattr(art, 'score', float(score))
            selected.append(art)  # ç›´æ¥æ·»åŠ articleå¯¹è±¡
            selected_pmids.add(art.pmid)  # è®°å½•å·²é€‰PMID
            added_count += 1

        batch_time = time.time() - batch_start_time
        logger.info(f"âœ… Batch {batch_count} completed in {batch_time:.2f}s (added {added_count} articles)")
        logger.info(f"ğŸ“Š Current progress: {len(selected)}/{K_total} articles")

        # ï¼ˆå¯é€‰ï¼‰åŠ æ¸©ï¼šä¸‹ä¸€æ‰¹ç•¥å¾®å¢å¤§æ–°è¿‘åç½®ï¼ˆæ­¤å¤„ä¿ç•™æ¥å£ï¼‰  # å¯åœ¨æ­¤è°ƒæ•´lambda_decay
        # lambda_decay *= 1.05  # è‹¥å¸Œæœ›ä¸‹ä¸€æ‰¹æ›´åå‘è¿‘æœŸï¼Œå¯è§£å¼€

    def _final_key(article):  # å®šä¹‰æœ€ç»ˆæ’åºé”®å‡½æ•°
        try:
            return (-article.score, -int(str(article.pubdate)[:4]), int(article.pmid))  # å…ˆæŒ‰åˆ†æ•°é™åºï¼Œå†æŒ‰å¹´ä»½é™åºï¼Œå†æŒ‰PMIDå‡åº
        except Exception:
            return (-getattr(article, 'score', 0), 0, 10**12)  # å…œåº•é”®

    logger.info("\nğŸ”„ Performing final sorting and cleanup...")
    selected.sort(key=_final_key)  # å¯¹æœ€ç»ˆç»“æœæ’åº
    
    total_time = time.time() - start_time
    logger.info(f"\nâœ¨ Search completed in {total_time:.2f}s")
    logger.info(f"ğŸ“Š Final statistics:")
    logger.info(f"  - Total articles found: {len(selected)}")
    years = [int(str(a.pubdate)[:4]) for a in selected]
    logger.info(f"  - Years covered: {min(years)} - {max(years)}")
    logger.info(f"  - Average score: {sum(getattr(a, 'score', 0) for a in selected)/len(selected):.3f}")
    
    return selected[:K_total]  # è¿”å›å‰K_totalæ¡

# -------------------- 7) ä»è‡ªç„¶è¯­è¨€åˆ°æ‰¹æ¬¡æ£€ç´¢çš„ä¸€é”®å°è£… --------------------
def batch_search_reviews_from_user_query(  # å…¥å£å‡½æ•°ï¼šè‡ªç„¶è¯­è¨€â†’æ£€ç´¢ç»“æœ
    user_query: str,  # è‡ªç„¶è¯­è¨€é—®é¢˜
    years_back: int = 10,  # è¿‘Nå¹´ä½œä¸ºæœç´¢æ± 
    batch_size: int = 60,  # æ¯æ‰¹å€™é€‰é…é¢
    topk_batch: int = 10,  # æ¯æ‰¹äº§å‡ºæ•°é‡
    K_total: int = 30,  # æ€»äº§å‡ºæ•°é‡
    strong_review: bool = False,  # æ˜¯å¦åŒ…å«ç³»ç»Ÿç»¼è¿°/Metaåˆ†æ
    lang_filter: Optional[List[str]] = None,  # è¯­è¨€è¿‡æ»¤
    lambda_decay: float = 0.35,  # å¹´ä»½æŒ‡æ•°è¡°å‡å‚æ•°
    year_max_ratio: float = 0.4  # å•å¹´å æ¯”ä¸Šé™
):
    logger.info("\n" + "="*80)
    logger.info("ğŸš€ Starting PubMed Review Search")
    logger.info("="*80)
    logger.info(f"ğŸ“ Query: {user_query}")
    logger.info(f"ğŸ“Š Search parameters:")
    logger.info(f"  - Years back: {years_back}")
    logger.info(f"  - Target articles: {K_total}")
    logger.info(f"  - Strong review only: {strong_review}")
    logger.info(f"  - Language filter: {lang_filter}")
    
    start_time = time.time()
    logger.info("\nğŸ¤– Step 1/2: Generating PubMed query...")
    base_query = llm_query_from_user_question(user_query)  # è°ƒLLMç”ŸæˆåŸºç¡€æ£€ç´¢å¼
    if "review[publication type]" not in base_query.lower():  # è‹¥LLMæ¼äº†Reviewé™å®š
        base_query = f"({base_query}) AND Review[Publication Type]"  # è‡ªåŠ¨è¡¥ä¸Šç»¼è¿°é™å®š
        logger.info("âœ… Added Review[Publication Type] filter")
    
    logger.info("\nğŸ” Step 2/2: Executing batch search...")
    results = batch_search_reviews(  # è°ƒç”¨æ ¸å¿ƒæ£€ç´¢å‡½æ•°
        base_query=base_query,  # ä¼ å…¥åŸºç¡€æ£€ç´¢å¼
        years_back=years_back,  # å¹´ä»½èŒƒå›´
        batch_size=batch_size,  # æ‰¹æ¬¡é…é¢
        topk_batch=topk_batch,  # æ‰¹å†…TopK
        K_total=K_total,  # æ€»é‡
        strong_review=strong_review,  # å¼ºç»¼è¿°å¼€å…³
        lang_filter=lang_filter,  # è¯­è¨€è¿‡æ»¤
        lambda_decay=lambda_decay,  # è¡°å‡å‚æ•°
        year_max_ratio=year_max_ratio  # å•å¹´å æ¯”
    )  # è¿”å›ç»“æ„åŒ–ç»“æœåˆ—è¡¨

    total_time = time.time() - start_time
    logger.info("\n" + "="*80)
    logger.info("âœ¨ Search Complete!")
    logger.info("="*80)
    logger.info(f"ğŸ“Š Found {len(results)}/{K_total} requested articles in {total_time:.2f}s")
    if results:
        years = [int(str(r.pubdate)[:4]) for r in results]
        logger.info(f"ğŸ“… Year range: {min(years)} - {max(years)}")
        logger.info(f"ğŸ“ˆ Score range: {min(getattr(r, 'score', 0) for r in results):.3f} - {max(getattr(r, 'score', 0) for r in results):.3f}")
        if len(results) < K_total:
            logger.warning(f"âš ï¸ Note: Only found {len(results)} articles, less than requested {K_total}")
    logger.info("="*80)
    
    return results

# -------------------- 8) ç¤ºä¾‹ --------------------
if __name__ == "__main__":  # ä»…è„šæœ¬ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œ
    print("\n" + "="*80)
    print("ğŸš€ PubMed Review Search Demo")
    print("="*80 + "\n")

    user_query = "Causal mechanisms linking diabetes and cardiovascular disease and potential therapeutic targets"  # ç¤ºä¾‹è‡ªç„¶è¯­è¨€é—®é¢˜
    print(f"ğŸ” Query: {user_query}\n")

    results = batch_search_reviews_from_user_query(  # è°ƒç”¨ä¸€é”®æ£€ç´¢
        user_query=user_query,  # ä¼ å…¥é—®é¢˜
        years_back=5,           # è¿‘5å¹´
        batch_size=60,          # æ¯æ‰¹å€™é€‰
        topk_batch=10,          # æ¯æ‰¹å–10
        K_total=30,             # æ€»å…±è¦30
        strong_review=False,    # æ˜¯å¦ä½¿ç”¨å¼ºç»¼è¿°ï¼šæ­¤å¤„å¦
        lang_filter=["english"],# åªè¦è‹±æ–‡æ–‡çŒ®ï¼ˆå¯å»æ‰æ­¤å‚æ•°ï¼‰
        lambda_decay=0.35,      # è¶Šå¤§è¶Šåè¿‘æœŸ
        year_max_ratio=0.4,     # å•å¹´æœ€å¤šå 40%
    )

    print("\n" + "="*80)
    print("ğŸ“Š SEARCH RESULTS")
    print("="*80)
    print(f"\nFound {len(results)} articles in total")
    
    if results:
        print(f"\nğŸ¯ ALL {len(results)} RESULTS:")
        print("-" * 80)
        for i, r in enumerate(results, 1):
            print(f"\n{i}. {r.title}")
            print(f"   PMID: {r.pmid} | Year: {int(str(r.pubdate)[:4])} | Score: {getattr(r, 'score', 0):.3f}")
            print(f"   Journal: {r.journal}")
            if i % 10 == 0 and i < len(results):  # æ¯10æ¡ç»“æœæ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿
                print("\n" + "-" * 40 + f" Result {i}/{len(results)} " + "-" * 40)
    else:
        print("\nâŒ No results found")

    print("\n" + "="*80)
    print("âœ¨ Demo completed!")
    print("="*80)